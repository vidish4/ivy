{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import threading\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapping_genre(genre,ll):\n",
    "    data = []\n",
    "    print(genre)\n",
    "    url_genre = ll\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    # response = requests.get(url)\n",
    "    response_genre = requests.get(url_genre, headers=headers)\n",
    "    soup_genre = BeautifulSoup(response_genre.text,'html.parser')\n",
    "    # print(soup)# Prettify the HTML\n",
    "    pretty_html = soup_genre.prettify()\n",
    "    list_class = soup_genre.find_all('li',class_='ipc-metadata-list-summary-item')\n",
    "    list_class_size = 0\n",
    "    for i in list_class:\n",
    "        list_class_size = list_class_size + 1\n",
    "    for i in range(0,min(20,list_class_size)):\n",
    "        print(i)\n",
    "        title_link = list_class[i].find('a', class_='ipc-title-link-wrapper')\n",
    "        movie_link = \"https://www.imdb.com\" + title_link['href']\n",
    "        url_movie = movie_link\n",
    "        # response = requests.get(url)\n",
    "        response_movie = requests.get(url_movie, headers=headers)\n",
    "        soup_movie = BeautifulSoup(response_movie.text,'html.parser')\n",
    "        # print(soup)# Prettify the HTML\n",
    "        # pretty_html = soup_movie.prettify()\n",
    "        title = soup_movie.find('title')\n",
    "        ratings = soup_movie.find('span',class_='sc-bde20123-1 cMEQkK')\n",
    "        title_cast = soup_movie.find('section', {'data-testid': 'title-cast'})\n",
    "        if title_cast:\n",
    "            title_cast_item = title_cast.find_all('div', {'data-testid':'title-cast-item'})\n",
    "\n",
    "        directors_writers = soup_movie.find('ul',class_='ipc-metadata-list ipc-metadata-list--dividers-all sc-bfec09a1-8 bHYmJY ipc-metadata-list--base')\n",
    "        # print(directors_writers)\n",
    "        if directors_writers:\n",
    "            dw = directors_writers.find_all('div',class_='ipc-metadata-list-item__content-container')\n",
    "            l = 0\n",
    "            for i in dw:\n",
    "                l =l+1\n",
    "            directors = []\n",
    "            writers = []\n",
    "            if l > 0:\n",
    "                directors = dw[0].find_all('li',class_='ipc-inline-list__item')\n",
    "            if l > 1:\n",
    "                writers = dw[1].find_all('li',class_='ipc-inline-list__item')\n",
    "\n",
    "        match = re.search(r'tt(\\d+)', title_link['href'])\n",
    "        tt = 'tt' + match.group(1)\n",
    "        story_link = \"https://www.imdb.com/title/\" + tt + \"/plotsummary/?ref_=tt_ov_pl\"\n",
    "        # print(story_link)\n",
    "        url_story = story_link\n",
    "        # response = requests.get(url)\n",
    "        response_story = requests.get(url_story, headers=headers)\n",
    "        soup_story = BeautifulSoup(response_story.text,'html.parser')\n",
    "        # print(soup)# Prettify the HTML\n",
    "        pretty_html_story = soup_story.prettify()\n",
    "\n",
    "        story = soup_story.find_all('div',class_=\"ipc-html-content-inner-div\")\n",
    "        # print(story[1].text)\n",
    "\n",
    "        user_review = soup_movie.find('section', {'data-testid': 'UserReviews'})\n",
    "        if user_review:\n",
    "            user_review_link = user_review.find('a',class_='ipc-title-link-wrapper')\n",
    "            # print(user_review_link['href'])\n",
    "            if user_review_link:\n",
    "                url_reviews = \"https://www.imdb.com\" + user_review_link['href']\n",
    "                # response = requests.get(url)\n",
    "                response_reviews = requests.get(url_reviews, headers=headers)\n",
    "                soup_reviews = BeautifulSoup(response_reviews.text,'html.parser')\n",
    "                # print(soup)# Prettify the HTML\n",
    "                pretty_html_reviews = soup_reviews.prettify()\n",
    "\n",
    "                reviews = soup_reviews.find('div',class_='lister-list')\n",
    "                # reviews.prettify()\n",
    "                rws_title = reviews.find_all('a',class_='title')\n",
    "                rws_user = reviews.find_all('span',class_='display-name-link')\n",
    "                user_ratings = reviews.find_all('div',class_='review-container')\n",
    "                user_ratings_ = []\n",
    "                for i in user_ratings:\n",
    "                    j = i.find('span',class_=\"rating-other-user-rating\")\n",
    "                    if j:\n",
    "                        s = str(j.text)\n",
    "                        s_mod = s.replace(\"\\n\", \"\")\n",
    "                        user_ratings_.append(s_mod)\n",
    "                    else:\n",
    "                        user_ratings_.append(\"NA\")\n",
    "                user_date = reviews.find_all('span',class_='review-date')\n",
    "                # print()\n",
    "                user_date_ = []\n",
    "                for i in user_date:\n",
    "                    user_date_.append(i.text)\n",
    "                rws = reviews.find_all('div',class_='text show-more__control')\n",
    "\n",
    "        entry = {'Title':title.text, 'Ratings': '', 'Cast':'' ,'Directors':'', 'Writers':'', 'Plot Summary': ''}\n",
    "        if not ratings:\n",
    "            entry['Ratings'] = '0.0'\n",
    "        else:\n",
    "            entry['Ratings'] = ratings.text\n",
    "        for i in title_cast_item:\n",
    "            actor = i.find('a', {'data-testid': 'title-cast-item__actor'})\n",
    "            entry['Cast'] = entry['Cast'] + actor.text + '|'\n",
    "        entry['Cast'] = entry['Cast'][:-1]\n",
    "        for i in directors:\n",
    "            entry['Directors'] = entry['Directors'] + i.text + '|'\n",
    "        entry['Directors'] = entry['Directors'][:-1]\n",
    "        for i in writers:\n",
    "            entry['Writers'] = entry['Writers'] + i.text + '|'\n",
    "        entry['Writers'] = entry['Writers'][:-1]\n",
    "\n",
    "        # for j in story:\n",
    "        #     entry['Plot Summary'] = j.text\n",
    "        entry['Plot Summary'] = story[0].text\n",
    "                \n",
    "        l = 0\n",
    "        for i in rws:\n",
    "            l = l+1\n",
    "        st = \"User Review \"\n",
    "        for i in range(0,min(10,l)):\n",
    "            final_review = rws_title[i].text[:-1] + ': ' + rws[i].text + \"~\" + rws_user[i].text + \" rated \" + user_ratings_[i] + \" on \" + user_date_[i]\n",
    "            final_review = final_review.replace('\\n','')\n",
    "            # print(final_review)\n",
    "            entry[st + str(i+1)] = final_review\n",
    "\n",
    "        if l < 10:\n",
    "            for i in range(l,10):\n",
    "                entry[st + str(i+1)] = 'Not Available'\n",
    "\n",
    "        # print(entry['User Review 10'])\n",
    "        data.append(entry)\n",
    "\n",
    "    file_name =  str(genre) +'.csv'\n",
    "    with open(file_name, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['Title', 'Ratings', 'Cast', 'Directors','Writers','Plot Summary',\"User Review 1\",\"User Review 2\", \"User Review 3\",\"User Review 4\",\"User Review 5\",\"User Review 6\",\"User Review 7\",\"User Review 8\",\"User Review 9\",\"User Review 10\"]\n",
    "        csv_writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # Write the header\n",
    "        csv_writer.writeheader()\n",
    "\n",
    "        # Write the data rows\n",
    "        csv_writer.writerows(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.imdb.com/feature/genre#movie\"\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "# response = requests.get(url)\n",
    "response = requests.get(url, headers=headers)\n",
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "# print(soup)# Prettify the HTML\n",
    "pretty_html = soup.prettify()\n",
    "\n",
    "movies = soup.find_all('div', class_='ipc-chip-list__scroller')\n",
    "\n",
    "refs = {}\n",
    "for i in movies[1]:\n",
    "    span_element = i.find('span', class_='ipc-chip__text')\n",
    "    # print(span_element.text)\n",
    "    refs[span_element.text] = \"https://www.imdb.com/\" + i['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = []\n",
    "for i in refs:\n",
    "    thread = threading.Thread(target=scrapping_genre,args=(i,refs[i]))\n",
    "    threads.append(thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action\n",
      "Adventure\n",
      "Animation\n",
      "Biography\n",
      "Comedy\n",
      "Crime\n",
      "Documentary\n",
      "Drama\n",
      "Family\n",
      "Fantasy\n",
      "Film-Noir\n",
      "History\n",
      "Horror\n",
      "Music\n",
      "Musical\n",
      "Mystery\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romance\n",
      "Sci-Fi\n",
      "Short\n",
      "Sport\n",
      "Thriller\n",
      "War\n",
      "Western\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "00\n",
      "\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "11\n",
      "\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "33\n",
      "\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "55\n",
      "5\n",
      "\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "66\n",
      "\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "66\n",
      "\n",
      "6\n",
      "66\n",
      "\n",
      "6\n",
      "6\n",
      "6\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "7\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "8\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "99\n",
      "\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "99\n",
      "\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "9\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "10\n",
      "1011\n",
      "\n",
      "10\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "11\n",
      "11\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "12\n",
      "13\n",
      "1212\n",
      "\n",
      "13\n",
      "13\n",
      "13\n",
      "12\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "14\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "13\n",
      "1314\n",
      "\n",
      "14\n",
      "13\n",
      "14\n",
      "14\n",
      "14\n",
      "13\n",
      "13\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "14\n",
      "14\n",
      "14\n",
      "15\n",
      "14\n",
      "15\n",
      "15\n",
      "1415\n",
      "\n",
      "14\n",
      "14\n",
      "15\n",
      "14\n",
      "1414\n",
      "\n",
      "14\n",
      "15\n",
      "16\n",
      "15\n",
      "14\n",
      "15\n",
      "16\n",
      "16\n",
      "15\n",
      "15\n",
      "15\n",
      "15\n",
      "16\n",
      "15\n",
      "15\n",
      "16\n",
      "15\n",
      "16\n",
      "16\n",
      "15\n",
      "15\n",
      "17\n",
      "1515\n",
      "\n",
      "1616\n",
      "\n",
      "16\n",
      "17\n",
      "16\n",
      "17\n",
      "17\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "17\n",
      "16\n",
      "16\n",
      "15\n",
      "17\n",
      "17\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "17\n",
      "17\n",
      "17\n",
      "18\n",
      "17\n",
      "17\n",
      "18\n",
      "17\n",
      "18\n",
      "17\n",
      "17\n",
      "1718\n",
      "\n",
      "18\n",
      "17\n",
      "18\n",
      "16\n",
      "18\n",
      "17\n",
      "17\n",
      "17\n",
      "18\n",
      "19\n",
      "18\n",
      "1817\n",
      "\n",
      "18\n",
      "18\n",
      "19\n",
      "18\n",
      "19\n",
      "18\n",
      "18\n",
      "19\n",
      "1818\n",
      "\n",
      "19\n",
      "17\n",
      "19\n",
      "19\n",
      "18\n",
      "18\n",
      "19\n",
      "18\n",
      "19\n",
      "19\n",
      "19\n",
      "18\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "18\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n",
      "19\n"
     ]
    }
   ],
   "source": [
    "# Start all threads\n",
    "start_time = time.time()\n",
    "\n",
    "for thread in threads:\n",
    "    thread.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "for thread in threads:\n",
    "    thread.join()\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrapping_genre('Documentary',refs['Documentary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ivy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
